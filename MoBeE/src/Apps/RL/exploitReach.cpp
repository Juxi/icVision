#include <unistd.h>
#include <time.h>
#include "learner.h"
#include "util.h"
#include "rl_problem.h"

int main(int argc, char *argv[])
{
    // read command line params
    yarp::os::Property config;
    config.fromCommand(argc,argv);
    
    bool connectToMoBeE = true;
    if ( config.check("dontConnect") )
        connectToMoBeE = false;
    
    // instantiate reinforcement learner
    Learner learner(16,"icubSim","right_arm",connectToMoBeE);
    
    // initialize from file
    bool initialized = false;
    if ( config.check("file") ) {
        std::string fileName = config.find("file").asString().c_str();
        if (learner.loadStateFile(fileName))
            initialized = true;
        else {
            printf("File read failed!!!\n");
            return 0;
        }
    }
    if (!initialized) {
        printf("ERROR: Must load a file to exploit reaches! Use --file 'filename'\n");
        return 0;
    }
    
    learner.setStateFileName("exploit_arm_state.dat");      // contains internal learner state
    learner.setHistoryFileName("exploit_arm_history.dat");  // history of states visited, actions taken, and rewards received
    
   
    learner.setModelLearning(false);     // don't mess with the state transition probabilities
    learner.setLearning(false);         // don't do value iteration after actions complete
    
    learner.setDiscountFactor(0.999);
    
    
    // reach targets in task space
    RL_Problem_Set problems(&learner);
    problems.sampleInit( -0.4, -0.1, 0.0, 0.4, 0.0, 0.0, 0.05 );
    
    int count = 1;
    for ( RL_Problem_Set::iterator i=problems.begin(); i!=problems.end(); ++i)
    {
        RL_Problem* prob = *i;
        
        //printf("\n\n\nNew Reach Target: (%f %f %f)\n\n",prob->target.x(), prob->target.y(), prob->target.z());
        yarp::os::ConstString sph = learner.mobee.mkSphere(prob->target.x(), prob->target.y(), prob->target.z(), 0.02);
        
        int max_r_idx = learner.predictRewards(prob->target);
        learner.valueIteration();
        learner.writeStateFile();
        
        State* s = NULL;
        Action* a = NULL;
        ReachAction* reachAction = NULL;
        
        printf("\n\n------- reach reward at %d -------\n",max_r_idx);
        while ( true ) {
         
            s = learner.getDiscreteState(); if (!s) break;
            a = s->greedyAction(); if (!a) break;
            a->start( prob->target, &prob->numActions );
            
            while ( a->isRunning() ) {
                yarp::os::Time::delay(0.2);
                printf(".");
            }
            
            reachAction = dynamic_cast<ReachAction*>(a);
            if (reachAction) break;
        }
        
        learner.mobee.rmGeom(sph);
        
        //getchar();
    }
    
    printf("All finished\n");
    return 1;
}